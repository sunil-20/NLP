{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05464f9",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cb33ee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Sunil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\anaconda_2021\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda_2021\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda_2021\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\anaconda_2021\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda_2021\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda_2021\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda_2021\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda_2021\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda_2021\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda_2021\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda_2021\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import gensim\n",
    "import spacy \n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import gutenberg \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3cc5f",
   "metadata": {},
   "source": [
    "## Corpus cleaning and refining for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e39dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub('[\\[].*?[\\]]','', text)\n",
    "    text = re.sub(r'(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "#Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "#Dealing with chaptor indicators\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "\n",
    "#Parse the cleaned novel\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "\n",
    "#Group into sentences\n",
    "alice_sents = [[sent, 'Carroll'] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, 'Austen']for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DF\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns= ['text', 'author'])\n",
    "sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4f8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with stopwords and lemmatize the token\n",
    "\n",
    "for i, sentence in enumerate(sentences['text']):\n",
    "    sentences.loc[i, 'text'] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140590c",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9156abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows size 4 and 8\n",
    "model1 = gensim.models.Word2Vec(\n",
    "    sentences['text'],\n",
    "    workers= 4,\n",
    "    min_count= 1,\n",
    "    window=4,\n",
    "    sg=0,\n",
    "    sample = 1e-3,\n",
    "    vector_size = 100,\n",
    "    hs = 1e-3\n",
    ")\n",
    "\n",
    "model2 = gensim.models.Word2Vec(\n",
    "    sentences['text'],\n",
    "    workers= 4,\n",
    "    min_count= 1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample = 1e-3,\n",
    "    vector_size = 100,\n",
    "    hs = 1e-3\n",
    ")\n",
    "\n",
    "#vector size 200\n",
    "\n",
    "model3 = gensim.models.Word2Vec(\n",
    "    sentences['text'],\n",
    "    workers= 4,\n",
    "    min_count= 1,\n",
    "    window=4,\n",
    "    sg=0,\n",
    "    sample = 1e-3,\n",
    "    vector_size = 200,\n",
    "    hs = 1e-3\n",
    ")\n",
    "\n",
    "model4 = gensim.models.Word2Vec(\n",
    "    sentences['text'],\n",
    "    workers= 4,\n",
    "    min_count= 1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample = 1e-3,\n",
    "    vector_size = 200,\n",
    "    hs = 1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f2f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating numerical features\n",
    "\n",
    "word2vec_arr1 = np.zeros((sentences.shape[0],100))\n",
    "word2vec_arr2 = np.zeros((sentences.shape[0],100))\n",
    "word2vec_arr3 = np.zeros((sentences.shape[0],200))\n",
    "word2vec_arr4 = np.zeros((sentences.shape[0],200))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr1[i,:] = np.mean([model1.wv[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr2[i,:] = np.mean([model2.wv[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr3[i,:] = np.mean([model3.wv[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr4[i,:] = np.mean([model4.wv[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr1 = pd.DataFrame(word2vec_arr1)\n",
    "word2vec_arr2 = pd.DataFrame(word2vec_arr2)\n",
    "word2vec_arr3 = pd.DataFrame(word2vec_arr3)\n",
    "word2vec_arr4 = pd.DataFrame(word2vec_arr4)\n",
    "\n",
    "sentences1 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr1], axis=1)\n",
    "sentences1.dropna(inplace=True)\n",
    "\n",
    "sentences2 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr2], axis=1)\n",
    "sentences2.dropna(inplace=True)\n",
    "\n",
    "sentences3 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr3], axis=1)\n",
    "sentences3.dropna(inplace=True)\n",
    "\n",
    "sentences4 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr4], axis=1)\n",
    "sentences4.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942899a3",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51adaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42edbb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Model 1----------------------\n",
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.6982478909798832\n",
      "\n",
      "Test set score: 0.6916342412451362\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9944841012329656\n",
      "\n",
      "Test set score: 0.7937743190661478\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8552887735236859\n",
      "\n",
      "Test set score: 0.7748054474708171\n",
      "----------------------Model 2----------------------\n",
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.6982478909798832\n",
      "\n",
      "Test set score: 0.6916342412451362\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9944841012329656\n",
      "\n",
      "Test set score: 0.7962062256809338\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8715120051914341\n",
      "\n",
      "Test set score: 0.8020428015564203\n",
      "----------------------Model 3----------------------\n",
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.6982478909798832\n",
      "\n",
      "Test set score: 0.6916342412451362\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9944841012329656\n",
      "\n",
      "Test set score: 0.7976653696498055\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8627514600908501\n",
      "\n",
      "Test set score: 0.8005836575875487\n",
      "----------------------Model 4----------------------\n",
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.6982478909798832\n",
      "\n",
      "Test set score: 0.6916342412451362\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9944841012329656\n",
      "\n",
      "Test set score: 0.813715953307393\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8643737832576249\n",
      "\n",
      "Test set score: 0.7962062256809338\n"
     ]
    }
   ],
   "source": [
    "Y1 = sentences1['author']\n",
    "Y2 = sentences2['author']\n",
    "Y3 = sentences3['author']\n",
    "Y4 = sentences4['author']\n",
    "\n",
    "X1 = np.array(sentences1.drop(['text','author'], 1))\n",
    "X2 = np.array(sentences2.drop(['text','author'], 1))\n",
    "X3 = np.array(sentences3.drop(['text','author'], 1))\n",
    "X4 = np.array(sentences4.drop(['text','author'], 1))\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.4, random_state=44)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.4, random_state=44)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, Y3, test_size=0.4, random_state=44)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, Y4, test_size=0.4, random_state=44)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "print('----------------------Model 1----------------------')\n",
    "lr.fit(X_train1, y_train1)\n",
    "rfc.fit(X_train1, y_train1)\n",
    "gbc.fit(X_train1, y_train1)\n",
    "print('----------------------Logistic Regression Scores----------------------')\n",
    "print('Training set score:', lr.score(X_train1, y_train1))\n",
    "print('\\nTest set score:', lr.score(X_test1, y_test1))\n",
    "\n",
    "print('----------------------Random Forest Scores----------------------')\n",
    "print('Training set score:', rfc.score(X_train1, y_train1))\n",
    "print('\\nTest set score:', rfc.score(X_test1, y_test1))\n",
    "\n",
    "print('----------------------Gradient Boosting Scores----------------------')\n",
    "print('Training set score:', gbc.score(X_train1, y_train1))\n",
    "print('\\nTest set score:', gbc.score(X_test1, y_test1))\n",
    "\n",
    "\n",
    "print('----------------------Model 2----------------------')\n",
    "lr.fit(X_train2, y_train2)\n",
    "rfc.fit(X_train2, y_train2)\n",
    "gbc.fit(X_train2, y_train2)\n",
    "print('----------------------Logistic Regression Scores----------------------')\n",
    "print('Training set score:', lr.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', lr.score(X_test2, y_test2))\n",
    "\n",
    "print('----------------------Random Forest Scores----------------------')\n",
    "print('Training set score:', rfc.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', rfc.score(X_test2, y_test2))\n",
    "\n",
    "print('----------------------Gradient Boosting Scores----------------------')\n",
    "print('Training set score:', gbc.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', gbc.score(X_test2, y_test2))\n",
    "\n",
    "print('----------------------Model 3----------------------')\n",
    "lr.fit(X_train3, y_train3)\n",
    "rfc.fit(X_train3, y_train3)\n",
    "gbc.fit(X_train3, y_train3)\n",
    "print('----------------------Logistic Regression Scores----------------------')\n",
    "print('Training set score:', lr.score(X_train3, y_train3))\n",
    "print('\\nTest set score:', lr.score(X_test3, y_test3))\n",
    "\n",
    "print('----------------------Random Forest Scores----------------------')\n",
    "print('Training set score:', rfc.score(X_train3, y_train3))\n",
    "print('\\nTest set score:', rfc.score(X_test3, y_test3))\n",
    "\n",
    "print('----------------------Gradient Boosting Scores----------------------')\n",
    "print('Training set score:', gbc.score(X_train3, y_train3))\n",
    "print('\\nTest set score:', gbc.score(X_test3, y_test3))\n",
    "\n",
    "print('----------------------Model 4----------------------')\n",
    "lr.fit(X_train4, y_train4)\n",
    "rfc.fit(X_train4, y_train4)\n",
    "gbc.fit(X_train4, y_train4)\n",
    "print('----------------------Logistic Regression Scores----------------------')\n",
    "print('Training set score:', lr.score(X_train4, y_train4))\n",
    "print('\\nTest set score:', lr.score(X_test4, y_test4))\n",
    "\n",
    "print('----------------------Random Forest Scores----------------------')\n",
    "print('Training set score:', rfc.score(X_train4, y_train4))\n",
    "print('\\nTest set score:', rfc.score(X_test4, y_test4))\n",
    "\n",
    "print('----------------------Gradient Boosting Scores----------------------')\n",
    "print('Training set score:', gbc.score(X_train4, y_train4))\n",
    "print('\\nTest set score:', gbc.score(X_test4, y_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cb641",
   "metadata": {},
   "source": [
    "## Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe870c",
   "metadata": {},
   "source": [
    "Overall GBC showed better performance compared to other models.\n",
    "GBC also did better in model 3. \n",
    "RFC's overall performance showed overfitting.\n",
    "Overall, considering overfitting, model 3 is the best performer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
